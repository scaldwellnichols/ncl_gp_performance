{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pathlib as path\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import geopandas as gpd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import contextily as ctx\n",
    "\n",
    "from src import parameters as params\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data provided. \n",
    "Update the parameters file to specify the location of the data files locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(params.DATA_DIR / params.data_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is GP level, with one record for each GP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df) == len(df.gp_code.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the fields in the dataset, we derive a variety of performance metrics.\n",
    "\n",
    "For each metric, we append a `(key, value)` pair to the dictionary `params.column_display_names` specifying the formatting of the column name for use in data visualisations and tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the number of appointments each GP offered in the month of September 2024 per GP (Full Time Equivalent) employed by the practice. \n",
    "\n",
    "In some cases the number of GPs < 1 (due to some GPs working part-time or a data quality issue). This causes the number of appointments per gp to be greater than the number of appointments and causes outliers.\n",
    "\n",
    "To handle this, we replace any outliers (appts_per_gp > Q3 + 1.5*IQR) with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['patients_per_gp'] = df['numberofpatients'] / (np.ceil(df['qualified_gp']) + np.ceil(df['training_gp']))\n",
    "params.column_display_names['patients_per_gp'] = 'Patients per GP'\n",
    "\n",
    "\n",
    "df['appts_per_gp'] = (df['AttendanceOutcome_Attended']) / (np.ceil(df['qualified_gp']) + np.ceil(df['training_gp']))\n",
    "#replace outliers with median\n",
    "df['appts_per_gp'] = df['appts_per_gp'].apply(lambda x: df['appts_per_gp'].median() if x > df['appts_per_gp'].quantile(0.75) + 1.5*(df['appts_per_gp'].quantile(0.75) - df['appts_per_gp'].quantile(0.25)) else x)\n",
    "params.column_display_names['appts_per_gp'] = 'Appointments per GP'\n",
    "\n",
    "efficiency_metrics = ['appts_per_gp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4)\n",
    "sns.boxplot(df.patients_per_gp.rename(params.column_display_names['patients_per_gp']), ax=ax[3])\n",
    "sns.boxplot(df.appts_per_gp.rename(params.column_display_names['appts_per_gp']), ax=ax[0])\n",
    "sns.boxplot(df.training_gp.rename(params.column_display_names['training_gp']), ax=ax[1])\n",
    "sns.boxplot(df.qualified_gp.rename(params.column_display_names['qualified_gp']), ax=ax[2])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waiting times (Same day appointments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we obtain the percentage of attended appointments booked on the same day in September 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['same_day_appointment_percentage'] = df['BookingtoApptGap_SameDay'] / (df['AttendanceOutcome_Attended'] + df['AttendanceOutcome_Unknown'])\n",
    "params.column_display_names['same_day_appointment_percentage'] = 'Same-day Appointments (%)'\n",
    "\n",
    "waiting_times_metrics = ['same_day_appointment_percentage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digital Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of appointments that were delivered over the phone or video call in September 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['digital_access_percentage'] = (df['ApptModality_Telephone'] + df['ApptModality_VideoConferenceOnline']) / (df['AttendanceOutcome_Attended'] + df['AttendanceOutcome_Unknown'])\n",
    "params.column_display_names['digital_access_percentage'] = 'Digital Access (%)'\n",
    "\n",
    "digital_access_metrics = ['digital_access_percentage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attendance rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of appointments in September that were attended (not missed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['attendance_rate'] = df['AttendanceOutcome_Attended'] / (df['AttendanceOutcome_Attended'] + df['AttendanceOutcome_DNA'])\n",
    "params.column_display_names['attendance_rate'] = 'Attendance Rate (%)'\n",
    "\n",
    "attendance_metrics = ['attendance_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality and Outcomes Framework (QOF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing of QoF variables to be in the range $(0, 1)$ rather than percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['qof_total'] = df.Total_QoF / 100\n",
    "params.column_display_names['qof_total'] = 'QoF Total (%)'\n",
    "\n",
    "df['qof_hypertension'] = df.Hypertension / 100\n",
    "params.column_display_names['qof_hypertension'] = 'QoF Hypertension (%)'\n",
    "\n",
    "df['qof_breast_cancer_screening'] = df.BreastScreeningCancer / 100\n",
    "params.column_display_names['qof_breast_cancer_screening'] = 'QoF Breast Cancer Screening (%)'\n",
    "\n",
    "df['qof_child_vaccination'] = df.ChildVaccination / 100\n",
    "params.column_display_names['qof_child_vaccination'] = 'QoF Child Vaccination (%)'\n",
    "\n",
    "qof_metrics = ['qof_total', 'qof_hypertension', 'qof_child_vaccination', 'EmergencyPresentationsCancer', 'AntibioticPrescribing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient experience / satisfaction (GP Survey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No processing required for the GP survey columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_survey_metrics = ['overallexp', 'lastgpapptneeds', 'lastgpapptwait', 'localgpservicesreception', 'gpcontactoverall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CQC Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ordinal CQC variables, a numerical encoding is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqc_rating_encoding = {'Outstanding': 4, 'Good': 3, 'Requires improvement': 2, 'Inadequate': 1}\n",
    "\n",
    "cqc_rating_columns = ['overall', 'responsive', 'wellled', 'effective', 'caring', 'safe']\n",
    "\n",
    "cqc_metrics = []\n",
    "for column in cqc_rating_columns:\n",
    "    cqc_metrics.append(f'{column}_coded')\n",
    "    df[f'{column}_coded'] = df[column].apply(lambda x : cqc_rating_encoding[x] if x in cqc_rating_encoding.keys() else np.nan)\n",
    "    params.column_display_names[f'{column}_coded'] = params.column_display_names[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics = efficiency_metrics + waiting_times_metrics + digital_access_metrics + attendance_metrics + qof_metrics + gp_survey_metrics + cqc_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to just North Central London (NCL) ICB\n",
    "\n",
    "Filtering to just looking at GPs in NCL leaves us with 175 records, as specified in the Task Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncl = df[df.icb_code == params.ncl_icb]\n",
    "\n",
    "len(df_ncl) == 175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-criteria Decision Analysis (MCDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to derive a composite score for each GP in NCL based on the performance metrics listed above. \n",
    "We will then use this composite score to rank the GPs where a lower score indicates lower performance.\n",
    "More information and background on this technique can be found at:\n",
    "\n",
    "[An Introductory Guide to Multi-Criteria Decision Analysis (MCDA) - Government Analysis Function](https://analysisfunction.civilservice.gov.uk/policy-store/an-introductory-guide-to-mcda/).\n",
    "\n",
    "Let us define:\n",
    "\n",
    "$$\n",
    "C := \\sum f_i(x_i) w_i\n",
    "$$\n",
    "\n",
    "where $f_i$ is some normalisation function for metric $x_i$ and $w_i$ is a weight assigned to the metric.\n",
    "\n",
    "The weights $w_i$ are an indication of how important the metric is. Engagement with stakeholders can be used to determine the weights used. \n",
    "For example, the team at NCL ICB might be particularly interested in patient clinical outcomes, in which case we would assign a higher weight the Quality and Outcomes Framework (QoF) metrics.\n",
    "Similarly, if a stakeholder is most interested in efficiency, we could assign more weight to the appointments per GP metric we defined above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must deal with missing values for these performance metrics. We will impute the NCL median when a performance metric is unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncl[params.performance_metrics.keys()].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncl_median_imputed = df_ncl.copy(deep=True)\n",
    "df_ncl_median_imputed[list(params.performance_metrics.keys())] = df_ncl[params.performance_metrics.keys()].fillna(df_ncl[params.performance_metrics.keys()].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling / Normalisation of Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalise the performance metrics to all have the same scale. This is so that when we aggregate them to obtain a composite score, they all carry the same weight. We can then use our custom defined weights to specify metric importance.\n",
    "\n",
    "**Min-max normalization** is a scaling technique used to rescale a dataset so that all values fall within a specific range, typically between 0 and 1. It is particularly useful when you need to compare variables that are measured on different scales (e.g., percentages vs. counts).\n",
    "\n",
    "**Formula for Min-Max Normalization**\n",
    "For a given field $ y $ and GP $j$ in the dataset, the normalised value of $y$ for GP $j$ is given by:\n",
    "$$\n",
    "y_j' = \\frac{y_j - \\min(y)}{\\max(y) - \\min(y)}\n",
    "$$\n",
    "\n",
    "- $ y_i $: The original value.\n",
    "- $\\min(y) $: The minimum value of the field across all GPs in NCL.\n",
    "- $ \\max(x) $: The maximum value of the field across all GPs in NCL.\n",
    "- $ y_j' $: The normalized value.\n",
    "\n",
    "We want a higher score to be a good thing, so some metrics will need 'inverting'. For example, the percentage of people living with hypertension would need inverting (a lower percentage in this case indicates better performance). In this case, the formula is:\n",
    "$$\n",
    "y_j' = 1 - \\frac{y_j - \\min(y)}{\\max(y) - \\min(y)} = \\frac{\\max(y) - y_j}{\\max(y) - \\min(y)} \n",
    "$$\n",
    "\n",
    "The dictionary `params.performance_metrics` contains an `'invert'` key for each metric specifying whether the metric should be inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_metrics = []\n",
    "for i, (metric, metric_info) in enumerate(params.performance_metrics.items()):\n",
    "    normalised_metrics.append(f'{metric}_norm')\n",
    "    if metric_info['invert']:\n",
    "        df_ncl_median_imputed[f'{metric}_norm'] = 1 - utils.min_max_normalisation(df_ncl_median_imputed[metric])\n",
    "    else:\n",
    "        df_ncl_median_imputed[f'{metric}_norm'] = utils.min_max_normalisation(df_ncl_median_imputed[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the range for all the normalised metrics is $(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncl_median_imputed[normalised_metrics].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign weights to each performance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where I would seek stakeholder engagement to understand which performance measures we are most interested in. These would then be assigned more weight.\n",
    "\n",
    "For this analysis, I have given slightly more weight to efficiency metrics like 'Appointments per GP' and 'Attendance Rate'. \n",
    "Missed appointments are a significant cost burden to the NHS while the number of appointments provided by each GP is a good indicator of overall efficiency.\n",
    "\n",
    "Please see the parameters.py file for the full set of weights used in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Composite Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance_metrics = pd.DataFrame(params.performance_metrics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncl_median_imputed['performance_score'] = df_ncl_median_imputed[normalised_metrics].mul(list(df_performance_metrics.weight / df_performance_metrics.weight.sum()), axis = 1).sum(axis = 1)\n",
    "df_ncl_median_imputed['performance_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncl_gps_ranked = df_ncl_median_imputed.sort_values(by = 'performance_score', ascending = True).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowest performing GPs in NCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low_performers = df_ncl_gps_ranked[0:4][['gp_code', 'performance_score', 'IMD2019', 'patients_per_gp'] + normalised_metrics]\n",
    "df_low_performers.set_index('gp_code', inplace = True)\n",
    "\n",
    "df_low_performers['gp_name'] = df_low_performers.apply(lambda row : utils.get_gp_name(row.index), axis = 1)\n",
    "df_low_performers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Composite Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composite Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "plt.bar(df_ncl_gps_ranked.index, df_ncl_gps_ranked.performance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_ncl_median_imputed.performance_score, bins = 15)\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "plt.savefig(params.OUTPUTS_DIR / 'performance-score-distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radar Chart\n",
    "\n",
    "Used to visualise the performance metrics for the lowest performing GPs.\n",
    "These are compared to the maximum and average values of the performance metrics across NCL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot radar chart for a specific GP practice\n",
    "def plot_radar_chart(df):\n",
    "    # Metrics and values for the selected GP\n",
    "    metrics = normalised_metrics\n",
    "    metric_display_names = [params.column_display_names[metric] for metric, info in params.performance_metrics.items()]\n",
    "    # Prepare for radar chart\n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "\n",
    "    average_values = df_ncl_median_imputed[normalised_metrics].mean(axis=0).values # Average values for each metric in NCL\n",
    "    high_values = df_ncl_median_imputed[normalised_metrics].max(axis=0).values  # Maximum values for each metric in NCL\n",
    "    average_values = np.append(average_values, average_values[0])\n",
    "    high_values = np.append(high_values, high_values[0])\n",
    "\n",
    "    # Create the radar chart\n",
    "    fig, ax = plt.subplots(math.ceil(len(df)/2), 2, figsize=(25, 25), subplot_kw=dict(polar=True))\n",
    "    for i, gp_name in enumerate(df.index):\n",
    "        values = df.loc[gp_name, normalised_metrics].values\n",
    "        # Extend values to close the radar chart\n",
    "        values = np.append(values, values[0])\n",
    "        \n",
    "        #ax[i].title(f\"Performance Metrics for {gp_name}\", fontsize=14)\n",
    "        ax[i//2, i%2].fill(angles, high_values, color='green', alpha=0.2, label='Metric Max')\n",
    "        ax[i//2, i%2].fill(angles, average_values, color='blue', alpha=0.2, label='Metric Average')\n",
    "        ax[i//2, i%2].fill(angles, values, color='red', alpha=0.4, label=f'{gp_name}')\n",
    "        ax[i//2, i%2].set_yticks([])  # Remove radial ticks\n",
    "        ax[i//2, i%2].set_xticks(angles[:-1])  # Set metric labels\n",
    "        ax[i//2, i%2].set_xticklabels(metric_display_names, fontsize=20)\n",
    "        ax[i//2, i%2].legend(loc='upper left', bbox_to_anchor=(0.9, 1), fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(params.OUTPUTS_DIR / 'radar-chart.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Plot for a specific GP\n",
    "plot_radar_chart(df_low_performers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots of the Composite Score and Individual Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_boxplot = ['performance_score'] + list(params.performance_metrics.keys())\n",
    "metrics_to_boxplot.remove('overall_coded')\n",
    "fig, ax = plt.subplots(math.ceil(len(metrics_to_boxplot)/3), 3, figsize=(10,15))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_boxplot):\n",
    "    if metric == 'performance_score':\n",
    "        df_boxplot = df_ncl_median_imputed[metric].rename('Composite Performance Score')\n",
    "    else:\n",
    "        df_boxplot = df_ncl_median_imputed[metric].rename(params.column_display_names[metric])\n",
    "    sns.boxplot(df_boxplot, ax=ax[i//3, i%3], width = 0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(params.OUTPUTS_DIR / 'performance-boxplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Plot\n",
    "\n",
    "Plots GPs on a map.\n",
    "The colour of the point on the map gives an indication of the performance rating (composite score) for the practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WARNING: This cell may take a long time to run (approx. 3 minutes)\n",
    "\n",
    "# Geocode the postcodes\n",
    "geolocator = Nominatim(user_agent=\"gp_performance_mapper\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "\n",
    "df_ncl_median_imputed['location'] = df_ncl_median_imputed['postcode'].apply(geocode)\n",
    "df_ncl_median_imputed['point'] = df_ncl_median_imputed['location'].apply(lambda loc: tuple(loc.point) if loc else None)\n",
    "\n",
    "# Drop rows with missing geocoded data\n",
    "df_ncl_median_imputed = df_ncl_median_imputed.dropna(subset=['point'])\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df_ncl_median_imputed, geometry=gpd.points_from_xy(df_ncl_median_imputed['point'].apply(lambda x: x[1]), df_ncl_median_imputed['point'].apply(lambda x: x[0])))\n",
    "gdf.crs = \"EPSG:4326\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "gdf.plot(column='performance_score', ax=ax, legend=False, cmap='coolwarm', markersize=50)\n",
    "\n",
    "# Add basemap\n",
    "ctx.add_basemap(ax, crs=gdf.crs, source=ctx.providers.CartoDB.Positron)\n",
    "\n",
    "# Add colorbar with label\n",
    "sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=plt.Normalize(vmin=gdf['performance_score'].min(), vmax=gdf['performance_score'].max()))\n",
    "sm._A = []\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Performance Score')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('GP Performance Map')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "plt.savefig(params.OUTPUTS_DIR / 'performance-map.png')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.diverging_palette(230,20,as_cmap=True)\n",
    "\n",
    "#plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(df_ncl[['IMD2019', 'patients_per_gp'] + performance_metrics].corr(), cmap=cmap)\n",
    "#plt.tight_layout()\n",
    "\n",
    "plt.savefig(params.OUTPUTS_DIR / 'correlation-heatmap.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
